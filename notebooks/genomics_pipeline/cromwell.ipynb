{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Cromwell on AWS](https://docs.opendata.aws/genomics-workflows/)\n",
    "\n",
    "[Cromwell](https://github.com/broadinstitute/cromwell) is a Workflow Management System geared towards scientific workflows. Cromwell is open sourced under the [BSD 3-Clause license](https://github.com/broadinstitute/cromwell/blob/develop/LICENSE.txt).\n",
    "\n",
    "![Image of Cromwell](https://docs.opendata.aws/genomics-workflows/cromwell/images/cromwell-on-aws_infrastructure.png)\n",
    "\n",
    "### Initialize Notebook Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import base64\n",
    "import project_path # path to helper methods\n",
    "import pprint\n",
    "\n",
    "from lib import workshop\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "cfn = boto3.client('cloudformation')\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "\n",
    "key_name = 'cromwell'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Create S3 Bucket](https://docs.aws.amazon.com/AmazonS3/latest/gsg/CreatingABucket.html)\n",
    "\n",
    "We will create an S3 bucket that will be used throughout the workshop for storing our data.\n",
    "\n",
    "[s3.create_bucket](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.create_bucket) boto3 documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = workshop.create_bucket_name('genomics-')\n",
    "session.resource('s3').create_bucket(Bucket=bucket, CreateBucketConfiguration={'LocationConstraint': region})\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Create VPC](https://aws.amazon.com/vpc/)\n",
    "\n",
    "Amazon Virtual Private Cloud (Amazon VPC) lets you provision a logically isolated section of the AWS Cloud where you can launch AWS resources in a virtual network that you define. You have complete control over your virtual networking environment, including selection of your own IP address range, creation of subnets, and configuration of route tables and network gateways. You can use both IPv4 and IPv6 in your VPC for secure and easy access to resources and applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vpc, subnet, subnet2 = workshop.create_and_configure_vpc()\n",
    "vpc_id = vpc.id\n",
    "subnet_id = subnet.id\n",
    "subnet2_id = subnet2.id\n",
    "print(vpc_id)\n",
    "print(subnet_id)\n",
    "print(subnet2_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Create EC2 Keypair](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html)\n",
    "\n",
    "Amazon EC2 uses public–key cryptography to encrypt and decrypt login information. Public–key cryptography uses a public key to encrypt a piece of data, such as a password, then the recipient uses the private key to decrypt the data. The public and private keys are known as a key pair.\n",
    "\n",
    "[ec2_client.create_key_pair](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ec2.html#EC2.ServiceResource.create_key_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = ec2_client.describe_key_pairs(\n",
    "    KeyNames=[\n",
    "        key_name,\n",
    "    ],\n",
    ")\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'InvalidKeyPair.NotFound':\n",
    "        print ('Creating keypair: %s' % key_name)\n",
    "        # Create an SSH key to use when logging into instances.\n",
    "        outfile = open(key_name + '.pem','w')\n",
    "        key_pair = ec2.create_key_pair(KeyName=key_name)\n",
    "        KeyPairOut = str(key_pair.key_material)\n",
    "        outfile.write(KeyPairOut)\n",
    "        outfile.close()\n",
    "        os.chmod(key_name + '.pem', 400)\n",
    "    else:\n",
    "        print ('Keypair: %s already exists' % key_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Create a custom AMI for Cromwell](https://docs.opendata.aws/genomics-workflows/aws-batch/create-custom-ami/)\n",
    "\n",
    "In all cases, you will need a AMI ID for the AWS Batch Compute Resource AMI that you created using the [\"Create a Custom AMI\"](https://docs.opendata.aws/genomics-workflows/aws-batch/create-custom-ami/) guide! We do not provide a default value since for most genomics workloads, you will need to account for more storage than the default AWS Batch AMI provides. We will launch a [CloudFormation](https://aws.amazon.com/cloudformation/) template to generate the custom AMI for use with Cromwell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"https://console.aws.amazon.com/cloudformation/home?#/stacks/new?stackName=GenomicsWorkflow-AMI&templateURL=https://s3.amazonaws.com/aws-genomics-workflows/templates/create-genomics-ami/create-custom-ami-existing-vpc.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://s3.amazonaws.com/aws-genomics-workflows/templates/create-genomics-ami/create-custom-ami-existing-vpc.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat create-custom-ami-existing-vpc.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Launching the CloudFormation stacks](https://docs.opendata.aws/genomics-workflows/aws-batch/configure-aws-batch-cfn/)\n",
    "\n",
    "The link below provides a CloudFormation template to deploy a base AWS Batch environment for genomics workflows. The `Full Stack` template is self-contained and will create all of the AWS resources, including VPC network, security groups, etc. The template defaults to using two Availability Zones for deploying instances. If you need more than this, leverage the next template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"https://console.aws.amazon.com/cloudformation/home?#/stacks/new?stackName=GenomicsEnv-Batch&templateURL=https://s3.amazonaws.com/aws-genomics-workflows/templates/aws-genomics-root-novpc.template.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://s3.amazonaws.com/aws-genomics-workflows/templates/aws-genomics-root-novpc.template.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat aws-genomics-root-novpc.template.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Status of the CloudFormation template\n",
    "\n",
    "We want to get the status and outputs of the CloudFormation template as it completes.\n",
    "\n",
    "[cfn.describe_stacks](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/cloudformation.html#CloudFormation.Client.describe_stacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "response = cfn.describe_stacks(StackName='GenomicsEnv-Batch')\n",
    "\n",
    "print(response[\"Stacks\"][0][\"StackStatus\"] +'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Outputs from CloudFormation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = response[\"Stacks\"][0][\"Outputs\"]\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.DataFrame(outputs, columns=[\"OutputKey\", \"OutputValue\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Launch the Cromwell CloudFormation stack](https://docs.opendata.aws/genomics-workflows/cromwell/cromwell-aws-batch/)\n",
    "\n",
    "#### Cromwell Server\n",
    "To ensure the highest level of security, and robustness for long running workflows, it is recommended that you use an EC2 instance as your Cromwell server for submitting workflows to AWS Batch.\n",
    "\n",
    "A couple things to note:\n",
    "\n",
    "* This server does not need to be permanent. In fact, when you are not running workflows, you should stop or terminate the instance so that you are not paying for resources you are not using.\n",
    "\n",
    "* You can launch a Cromwell server just for yourself and exactly when you need it.\n",
    "\n",
    "* This server does not need to be in the same VPC as the one that Batch will launch instances in.\n",
    "\n",
    "#### Parameters\n",
    "When launching the CloudFormation template you will copy the `GenomicsEnvS3Bucket` value into the `S3BucketName` parameter and `GenomicsEnvDefaultJobQueueArn` value into the `BatchQueue` parameter under the `Cromwell Configuration` section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"https://console.aws.amazon.com/cloudformation/home?#/stacks/new?stackName=CromwellServer&templateURL=https://s3.amazonaws.com/aws-genomics-workflows/templates/cromwell/cromwell-server.template.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://s3.amazonaws.com/aws-genomics-workflows/templates/cromwell/cromwell-server.template.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat cromwell-server.template.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Status of Cromwell CloudFormation stack "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = cfn.describe_stacks(StackName='CromwellServer')\n",
    "\n",
    "print(response[\"Stacks\"][0][\"StackStatus\"] +'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Outputs from CloudFormation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = response[\"Stacks\"][0][\"Outputs\"]\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.DataFrame(outputs, columns=[\"OutputKey\", \"OutputValue\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile simple-hello.wdl\n",
    "\n",
    "task echoHello{\n",
    "    command {\n",
    "        echo \"Hello AWS!\"\n",
    "    }\n",
    "    runtime {\n",
    "        docker: \"ubuntu:latest\"\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "workflow printHelloAndGoodbye {\n",
    "    call echoHello\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST \"http://{{cromwell server}}/api/workflows/v1\" \\\n",
    "    -H \"accept: application/json\" \\\n",
    "    -F \"workflowSource=@simple-hello.wdl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real world example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile HaplotypeCaller.aws.wdl\n",
    "\n",
    "## Copyright Broad Institute, 2017\n",
    "##\n",
    "## This WDL workflow runs HaplotypeCaller from GATK4 in GVCF mode on a single sample\n",
    "## according to the GATK Best Practices (June 2016), scattered across intervals.\n",
    "##\n",
    "## Requirements/expectations :\n",
    "## - One analysis-ready BAM file for a single sample (as identified in RG:SM)\n",
    "## - Set of variant calling intervals lists for the scatter, provided in a file\n",
    "##\n",
    "## Outputs :\n",
    "## - One GVCF file and its index\n",
    "##\n",
    "## Cromwell version support\n",
    "## - Successfully tested on v29\n",
    "## - Does not work on versions < v23 due to output syntax\n",
    "##\n",
    "## IMPORTANT NOTE: HaplotypeCaller in GATK4 is still in evaluation phase and should not\n",
    "## be used in production until it has been fully vetted. In the meantime, use the GATK3\n",
    "## version for any production needs.\n",
    "##\n",
    "## Runtime parameters are optimized for Broad's Google Cloud Platform implementation.\n",
    "##\n",
    "## LICENSING :\n",
    "## This script is released under the WDL source code license (BSD-3) (see LICENSE in\n",
    "## https://github.com/broadinstitute/wdl). Note however that the programs it calls may\n",
    "## be subject to different licenses. Users are responsible for checking that they are\n",
    "## authorized to run all programs before running this script. Please see the dockers\n",
    "## for detailed licensing information pertaining to the included programs.\n",
    "\n",
    "# WORKFLOW DEFINITION\n",
    "workflow HaplotypeCallerGvcf_GATK4 {\n",
    "  File input_bam\n",
    "  File input_bam_index\n",
    "  File ref_dict\n",
    "  File ref_fasta\n",
    "  File ref_fasta_index\n",
    "  File scattered_calling_intervals_list\n",
    "\n",
    "  String gatk_docker\n",
    "\n",
    "  String gatk_path\n",
    "\n",
    "  Array[File] scattered_calling_intervals = read_lines(scattered_calling_intervals_list)\n",
    "\n",
    "  String sample_basename = basename(input_bam, \".bam\")\n",
    "\n",
    "  String gvcf_name = sample_basename + \".g.vcf.gz\"\n",
    "  String gvcf_index = sample_basename + \".g.vcf.gz.tbi\"\n",
    "\n",
    "  # Call variants in parallel over grouped calling intervals\n",
    "  scatter (interval_file in scattered_calling_intervals) {\n",
    "\n",
    "    # Generate GVCF by interval\n",
    "    call HaplotypeCaller {\n",
    "      input:\n",
    "        input_bam = input_bam,\n",
    "        input_bam_index = input_bam_index,\n",
    "        interval_list = interval_file,\n",
    "        gvcf_name = gvcf_name,\n",
    "        ref_dict = ref_dict,\n",
    "        ref_fasta = ref_fasta,\n",
    "        ref_fasta_index = ref_fasta_index,\n",
    "        docker_image = gatk_docker,\n",
    "        gatk_path = gatk_path\n",
    "    }\n",
    "  }\n",
    "\n",
    "  # Merge per-interval GVCFs\n",
    "  call MergeGVCFs {\n",
    "    input:\n",
    "      input_vcfs = HaplotypeCaller.output_gvcf,\n",
    "      vcf_name = gvcf_name,\n",
    "      vcf_index = gvcf_index,\n",
    "      docker_image = gatk_docker,\n",
    "      gatk_path = gatk_path\n",
    "  }\n",
    "\n",
    "  # Outputs that will be retained when execution is complete\n",
    "  output {\n",
    "    File output_merged_gvcf = MergeGVCFs.output_vcf\n",
    "    File output_merged_gvcf_index = MergeGVCFs.output_vcf_index\n",
    "  }\n",
    "}\n",
    "\n",
    "# TASK DEFINITIONS\n",
    "\n",
    "# HaplotypeCaller per-sample in GVCF mode\n",
    "task HaplotypeCaller {\n",
    "  File input_bam\n",
    "  File input_bam_index\n",
    "  String gvcf_name\n",
    "  File ref_dict\n",
    "  File ref_fasta\n",
    "  File ref_fasta_index\n",
    "  File interval_list\n",
    "  Int? interval_padding\n",
    "  Float? contamination\n",
    "  Int? max_alt_alleles\n",
    "\n",
    "  Int preemptible_tries\n",
    "  Int disk_size\n",
    "  String mem_size\n",
    "\n",
    "  String docker_image\n",
    "  String gatk_path\n",
    "  String java_opt\n",
    "\n",
    "  command {\n",
    "    ${gatk_path} --java-options ${java_opt} \\\n",
    "      HaplotypeCaller \\\n",
    "      -R ${ref_fasta} \\\n",
    "      -I ${input_bam} \\\n",
    "      -O ${gvcf_name} \\\n",
    "      -L ${interval_list} \\\n",
    "      -ip ${default=100 interval_padding} \\\n",
    "      -contamination ${default=0 contamination} \\\n",
    "      --max-alternate-alleles ${default=3 max_alt_alleles} \\\n",
    "      -ERC GVCF\n",
    "  }\n",
    "\n",
    "  runtime {\n",
    "    docker: docker_image\n",
    "    memory: mem_size\n",
    "    cpu: 1\n",
    "    disks: \"local-disk\"\n",
    "  }\n",
    "\n",
    "  output {\n",
    "    File output_gvcf = \"${gvcf_name}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "# Merge GVCFs generated per-interval for the same sample\n",
    "task MergeGVCFs {\n",
    "  Array [File] input_vcfs\n",
    "  String vcf_name\n",
    "  String vcf_index\n",
    "\n",
    "  Int preemptible_tries\n",
    "  Int disk_size\n",
    "  String mem_size\n",
    "\n",
    "  String docker_image\n",
    "  String gatk_path\n",
    "  String java_opt\n",
    "\n",
    "  command {\n",
    "    ${gatk_path} --java-options ${java_opt} \\\n",
    "      MergeVcfs \\\n",
    "      --INPUT=${sep=' --INPUT=' input_vcfs} \\\n",
    "      --OUTPUT=${vcf_name}\n",
    "  }\n",
    "\n",
    "  runtime {\n",
    "    docker: docker_image\n",
    "    memory: mem_size\n",
    "    cpu: 1\n",
    "    disks: \"local-disk\"\n",
    "}\n",
    "\n",
    "  output {\n",
    "    File output_vcf = \"${vcf_name}\"\n",
    "    File output_vcf_index = \"${vcf_index}\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile HaplotypeCaller.aws.json\n",
    "\n",
    "{\n",
    "  \"##_COMMENT1\": \"INPUT BAM\",\n",
    "  \"HaplotypeCallerGvcf_GATK4.input_bam\": \"s3://gatk-test-data/wgs_bam/NA12878_24RG_hg38/NA12878_24RG_small.hg38.bam\",\n",
    "  \"HaplotypeCallerGvcf_GATK4.input_bam_index\": \"s3://gatk-test-data/wgs_bam/NA12878_24RG_hg38/NA12878_24RG_small.hg38.bai\",\n",
    "\n",
    "  \"##_COMMENT2\": \"REFERENCE FILES\",\n",
    "  \"HaplotypeCallerGvcf_GATK4.ref_dict\": \"s3://broad-references/hg38/v0/Homo_sapiens_assembly38.dict\",\n",
    "  \"HaplotypeCallerGvcf_GATK4.ref_fasta\": \"s3://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta\",\n",
    "  \"HaplotypeCallerGvcf_GATK4.ref_fasta_index\": \"s3://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai\",\n",
    "\n",
    "  \"##_COMMENT3\": \"INTERVALS\",\n",
    "  \"HaplotypeCallerGvcf_GATK4.scattered_calling_intervals_list\": \"s3://gatk-test-data/intervals/hg38_wgs_scattered_calling_intervals.txt\",\n",
    "  \"HaplotypeCallerGvcf_GATK4.HaplotypeCaller.interval_padding\": 100,\n",
    "\n",
    "  \"##_COMMENT4\": \"DOCKERS\",\n",
    "  \"HaplotypeCallerGvcf_GATK4.gatk_docker\": \"broadinstitute/gatk:4.0.0.0\",\n",
    "\n",
    "  \"##_COMMENT5\": \"PATHS\",\n",
    "  \"HaplotypeCallerGvcf_GATK4.gatk_path\": \"/gatk/gatk\",\n",
    "\n",
    "  \"##_COMMENT6\": \"JAVA OPTIONS\",\n",
    "  \"HaplotypeCallerGvcf_GATK4.HaplotypeCaller.java_opt\": \"-Xms8000m\",\n",
    "  \"HaplotypeCallerGvcf_GATK4.MergeGVCFs.java_opt\": \"-Xms8000m\",\n",
    "\n",
    "  \"##_COMMENT7\": \"MEMORY ALLOCATION\",\n",
    "  \"HaplotypeCallerGvcf_GATK4.HaplotypeCaller.mem_size\": \"10 GB\",\n",
    "  \"HaplotypeCallerGvcf_GATK4.MergeGVCFs.mem_size\": \"30 GB\",\n",
    "\n",
    "  \"##_COMMENT8\": \"DISK SIZE ALLOCATION\",\n",
    "  \"HaplotypeCallerGvcf_GATK4.HaplotypeCaller.disk_size\": 100,\n",
    "  \"HaplotypeCallerGvcf_GATK4.MergeGVCFs.disk_size\": 100,\n",
    "\n",
    "  \"##_COMMENT9\": \"PREEMPTION\",\n",
    "  \"HaplotypeCallerGvcf_GATK4.HaplotypeCaller.preemptible_tries\": 3,\n",
    "  \"HaplotypeCallerGvcf_GATK4.MergeGVCFs.preemptible_tries\": 3\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit job to Cromwell server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST \"http://{{cromwell server}}/api/workflows/v1\" \\\n",
    "    -H  \"accept: application/json\" \\\n",
    "    -F \"workflowSource=@HaplotypeCaller.aws.wdl\" \\\n",
    "    -F \"workflowInputs=@HaplotypeCaller.aws.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = cfn.delete_stack(StackName='CromwellServer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = cfn.delete_stack(StackName='GenomicsEnv-Batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = cfn.delete_stack(StackName='GenomicsWorkflow-AMI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ec2_client.delete_key_pair(KeyName=key_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workshop.vpc_cleanup(vpc_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
