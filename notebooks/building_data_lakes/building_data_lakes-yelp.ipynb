{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Data Lake on AWS\n",
    "\n",
    "In this workshop we are going to be using Yelp reviews from 2015 from the AWS Open Data Registry [Yelp Reviews NLP Fast.ai](https://registry.opendata.aws/fast-ai-nlp/). We will start by uploading the Yelp review dataset untar and upload to S3 to register the raw data with the Glue Data Catalog. Once we have the data registered we will transform the data to get only the columns necessary to run an NLP job on the reviews to get sentiment. We will be using [Amazon Comprehend](https://aws.amazon.com/comprehend/) to get the sentiment of the reviews as an example of using the built-in APIs available from AWS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "import project_path # path to helper methods\n",
    "from lib import workshop\n",
    "\n",
    "glue = boto3.client('glue')\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "bucket = '{{s3 workshop bucket}}' # Bucket for all data used in this workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Download Yelp Reviews](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-example-download-file.html) \n",
    "\n",
    "We will download the reviews from the Fast.ai NLP dataset available on the [AWS Open Data Registry](https://registry.opendata.aws/fast-ai-nlp/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    s3.Bucket('fast-ai-nlp').download_file('yelp_review_full_csv.tgz', 'yelp_review_full_csv.tgz')\n",
    "except botocore.exceptions.ClientError as e:\n",
    "    if e.response['Error']['Code'] == \"404\":\n",
    "        print(\"The object does not exist.\")\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Untar Yelp Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvzf yelp_review_full_csv.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View raw csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "df = pd.read_csv('yelp_review_full_csv/train.csv', header=None)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Upload to S3](https://docs.aws.amazon.com/AmazonS3/latest/dev/Welcome.html)\n",
    "\n",
    "Next, we will upload the json file created above to S3 to be used later in the workshop.\n",
    "\n",
    "[s3.upload_file](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.upload_file) boto3 documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'train.csv'\n",
    "session.resource('s3').Bucket(bucket).Object(os.path.join('yelp', 'raw', file_name)).upload_file('yelp_review_full_csv/'+file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discover the data in your Data Lake\n",
    "\n",
    "In this next section we will be using [AWS Glue](https://aws.amazon.com/glue/) to discover, catalog, and transform your data.Glue currently only supports `Python 2.7`, hence we'll write the script in `Python 2.7`.\n",
    "\n",
    "### Permission setup for invoking AWS Glue from this Notebook\n",
    "In order to enable this Notebook to run AWS Glue jobs, we need to add one additional permission to the default execution role of this notebook. We will be using SageMaker Python SDK to retrieve the default execution role and then you have to go to [IAM Dashboard](https://console.aws.amazon.com/iam/home) to edit the Role to add AWS Glue specific permission. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding out the current execution role of the Notebook\n",
    "We are using SageMaker Python SDK to retrieve the current role for this Notebook which needs to be enhanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SageMaker Python SDK to get the Session and execution_role\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "role_name = role[role.rfind('/') + 1:]\n",
    "print(role_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding AWS Glue as an additional trusted entity to this role\n",
    "This step is needed if you want to pass the execution role of this Notebook while calling Glue APIs as well without creating an additional **Role**. If you have not used AWS Glue before, then this step is mandatory. \n",
    "\n",
    "If you have used AWS Glue previously, then you should have an already existing role that can be used to invoke Glue APIs. In that case, you can pass that role while calling Glue (later in this notebook) and skip this next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the IAM dashboard, please click on **Roles** on the left sidenav and search for this Role. Once the Role appears, click on the Role to go to its **Summary** page. Click on the **Trust relationships** tab on the **Summary** page to add AWS Glue as an additional trusted entity. \n",
    "\n",
    "Click on **Edit trust relationship** and replace the JSON with this JSON.\n",
    "```\n",
    "{\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Principal\": {\n",
    "        \"Service\": [\n",
    "          \"sagemaker.amazonaws.com\",\n",
    "          \"glue.amazonaws.com\"\n",
    "        ]\n",
    "      },\n",
    "      \"Action\": \"sts:AssumeRole\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "Once this is complete, click on **Update Trust Policy** and you are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"https://console.aws.amazon.com/iam/home?region={0}#/roles/{1}\".format(region, role_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create [Glue Catalog Database](https://docs.aws.amazon.com/glue/latest/dg/define-database.html)\n",
    "\n",
    "When you define a table in the AWS Glue Data Catalog, you add it to a database. A database is used to organize tables in AWS Glue. You can organize your tables using a crawler or using the AWS Glue console. A table can be in only one database at a time.\n",
    "\n",
    "There is a central Glue Catalog for each AWS account. When creating the database you will use your account id declared above as `account_id`\n",
    "\n",
    "[glue.create_database](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/glue.html#Glue.Client.create_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_name = 'yelp'\n",
    "\n",
    "response = glue.create_database(\n",
    "    CatalogId=account_id,\n",
    "    DatabaseInput={\n",
    "        'Name': database_name,\n",
    "        'Description': 'Database for Yelp Reviews'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Create the Raw table in Glue](https://docs.aws.amazon.com/glue/latest/dg/tables-described.html)\n",
    "\n",
    "When you define a table in AWS Glue, you also specify the value of a classification field that indicates the type and format of the data that's stored in that table. If a crawler creates the table, these classifications are determined by either a built-in classifier or a custom classifier. If you create a table manually in the console or by using an API, you specify the classification when you define the table. For more information about creating a table using the AWS Glue console, see [Working with Tables on the AWS Glue Console](https://docs.aws.amazon.com/glue/latest/dg/console-tables.html).\n",
    "\n",
    "[glue.create_table](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/glue.html#Glue.Client.create_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = 's3://'+bucket+'/yelp/raw'\n",
    "table_name = 'raw_reviews'\n",
    "\n",
    "response = glue.create_table(\n",
    "    CatalogId=account_id,\n",
    "    DatabaseName=database_name,\n",
    "    TableInput={\n",
    "        'Name': table_name,\n",
    "        'Description': 'Raw Yelp reviews dataset',\n",
    "        'StorageDescriptor': {\n",
    "            'Columns': [\n",
    "                {\n",
    "                    'Name': 'rating',\n",
    "                    'Type': 'tinyint',\n",
    "                    'Comment': 'Rating of from the Yelp review'\n",
    "                },\n",
    "                {\n",
    "                    'Name': 'review',\n",
    "                    'Type': 'string',\n",
    "                    'Comment': 'Review text of from the Yelp review'\n",
    "                }\n",
    "            ],\n",
    "            'Location': location,\n",
    "            'InputFormat': 'org.apache.hadoop.mapred.TextInputFormat',\n",
    "            'OutputFormat': 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',\n",
    "            'SerdeInfo': {\n",
    "                'SerializationLibrary': 'org.apache.hadoop.hive.serde2.OpenCSVSerde',\n",
    "                'Parameters': {\n",
    "                    'escapeChar': '\\\\',\n",
    "                    'separatorChar': ',',\n",
    "                    'serialization.format': '1'\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "        'TableType': 'EXTERNAL_TABLE'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Yelp Raw Reviews Data \n",
    "\n",
    "To see the raw Yelp reviews we will be installing a python library for querying the data in the Glue Data Catalog with Athena. More information about [PyAthena](https://pypi.org/project/PyAthena/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyAthena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyathena import connect\n",
    "from pyathena.util import as_pandas\n",
    "\n",
    "cursor = connect(region_name=region, s3_staging_dir='s3://'+bucket+'/yelp/temp').cursor()\n",
    "cursor.execute('select * from ' + database_name + '.' + table_name + ' limit 10')\n",
    "\n",
    "df = as_pandas(cursor)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Raw data to provide insights and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading the code and other dependencies to S3 for AWS Glue\n",
    "In order to run your code in AWS Glue, we need to upload the code and dependencies directly to S3 and pass those locations while invoking the Glue job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint as pp\n",
    "\n",
    "client = boto3.client('comprehend', region_name=region)\n",
    "\n",
    "response = client.detect_sentiment(Text='This is some example text', LanguageCode='en')\n",
    "pp.pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Sentiment of Yelp Review Data\n",
    "\n",
    "We will create a Pyspark job to add primary key and run them through [Amazon Comprehend](https://aws.amazon.com/comprehend/) to get sentiment analysis of the reviews. Replace `{region}` with the region this notebook is running in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile yelp_etl.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Row, Window, SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME', 'S3_OUTPUT_BUCKET', 'S3_OUTPUT_KEY_PREFIX', 'DATABASE_NAME', 'TABLE_NAME', 'REGION'])\n",
    "\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "# Covert Glue DynamocFrame to Spark DataFrame\n",
    "yelp = glueContext.create_dynamic_frame.from_catalog(database='yelp', table_name='raw_reviews')\n",
    "yelpDF = yelp.toDF()\n",
    "\n",
    "idxDF = yelpDF.select('rating', 'review').withColumn('id', monotonically_increasing_id())\n",
    "\n",
    "MIN_SENTENCE_LENGTH_IN_CHARS = 10 \n",
    "MAX_SENTENCE_LENGTH_IN_CHARS = 4500\n",
    "COMPREHEND_BATCH_SIZE = 25  ## This batch size results in groups no larger than 25 items\n",
    "NUMBER_OF_BATCHES = 40\n",
    "ROW_LIMIT = 10000 #Number of reviews we will process for this workshop\n",
    "\n",
    "## Each task handles 25*40 records, there should be 10 partitions overall to process 10000 records.\n",
    "ComprehendRow = Row(\"review_id\", \"sentiment\")\n",
    "def getBatchComprehend(input_list):\n",
    "  ## You can import the ratelimit module if you want to further rate limit API calls to Comprehend\n",
    "  ## https://pypi.org/project/ratelimit/\n",
    "  #from ratelimit import rate_limited\n",
    "  arr = []\n",
    "  bodies = [i[1] for i in input_list]\n",
    "  bodies = filter(None, bodies)\n",
    "  client = boto3.client('comprehend', region_name='{region}')\n",
    "\n",
    "  #@rate_limited(1) \n",
    "  def callSentimentApi(text_list):\n",
    "    response = ''\n",
    "    try:\n",
    "      response = client.batch_detect_sentiment(TextList = text_list, LanguageCode = 'en')\n",
    "    except:\n",
    "      pass\n",
    "    return response\n",
    "  \n",
    "  for i in range(NUMBER_OF_BATCHES-1):\n",
    "    text_list = bodies[COMPREHEND_BATCH_SIZE * i : COMPREHEND_BATCH_SIZE * (i+1)]\n",
    "    sent_resp = callSentimentApi(text_list)\n",
    "    \n",
    "    try:\n",
    "        for r in sent_resp['ResultList']:\n",
    "          idx = COMPREHEND_BATCH_SIZE * i + r['Index']\n",
    "          arr.append(ComprehendRow(input_list[idx][0], r['Sentiment']))\n",
    "    except:\n",
    "        print(text_list)\n",
    "        print(sent_resp)\n",
    "  return arr\n",
    "\n",
    "df = idxDF \\\n",
    "  .withColumn('review_len', F.length('review')) \\\n",
    "  .filter(F.col('review_len') > MIN_SENTENCE_LENGTH_IN_CHARS) \\\n",
    "  .filter(F.col('review_len') < MAX_SENTENCE_LENGTH_IN_CHARS) \\\n",
    "  .limit(ROW_LIMIT)\n",
    "\n",
    "record_count = df.count()\n",
    "print('record count=' + str(record_count))\n",
    "\n",
    "df2 = df \\\n",
    "  .repartition(record_count/(NUMBER_OF_BATCHES*COMPREHEND_BATCH_SIZE)) \\\n",
    "  .sortWithinPartitions(['id'], ascending=True)\n",
    "\n",
    " ## Concatenate submission id and body tuples into arrays of similar size\n",
    "group_rdd = df2.rdd.map(lambda l: (l.id, l.review.encode(\"utf-8\"))).glom()\n",
    "  \n",
    "transformed = group_rdd \\\n",
    "  .map(lambda l: getBatchComprehend(l)) \\\n",
    "  .flatMap(lambda x: x) \\\n",
    "  .toDF()\n",
    "\n",
    "## Join sentiment results with the review dataset\n",
    "joinedDF = df2 \\\n",
    "  .join(transformed, transformed.review_id == df2.id) \\\n",
    "  .drop(transformed.review_id)\n",
    "\n",
    "joinedDF = joinedDF \\\n",
    "  .repartition(1) \\\n",
    "  .sortWithinPartitions(['id'], ascending=True)\n",
    "\n",
    "joinedsink= DynamicFrame.fromDF(joinedDF, glueContext, \"joined\")\n",
    "parquet_output_path = 's3://' + os.path.join(args['S3_OUTPUT_BUCKET'], args['S3_OUTPUT_KEY_PREFIX'])\n",
    "datasink5 = glueContext.write_dynamic_frame.from_options(frame = joinedsink, connection_type = \"s3\", connection_options = {\"path\": parquet_output_path, \"partitionKeys\": [\"sentiment\"]}, format=\"parquet\", transformation_ctx=\"datasink5\")\n",
    "                                                                                                                      \n",
    "job.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the Yelp ETL script to S3\n",
    "We will be uploading the `github_etl.py` script to S3 now so that Glue can use it to run the PySpark job. You can replace it with your own script if needed. If your code has multiple files, you need to zip those files and upload to S3 instead of uploading a single file like it's being done here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_location = sess.upload_data(path='yelp_etl.py', bucket=bucket, key_prefix='yelp/codes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output location of the data. The input data will be split, transformed, and \n",
    "# uploaded to output/train and output/validation\n",
    "s3_output_bucket = bucket\n",
    "s3_output_key_prefix = 'yelp/parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling Glue APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll be creating Glue client via Boto so that we can invoke the `create_job` API of Glue. `create_job` API will create a job definition which can be used to execute your jobs in Glue. The job definition created here is mutable. While creating the job, we are also passing the code location as well as the dependencies location to Glue.\n",
    "\n",
    "`AllocatedCapacity` parameter controls the hardware resources that Glue will use to execute this job. It is measures in units of `DPU`. For more information on `DPU`, please see [here](https://docs.aws.amazon.com/glue/latest/dg/add-job.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "import time\n",
    "\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "job_name = 'yelp-etl-' + timestamp_prefix\n",
    "response = glue.create_job(\n",
    "    Name=job_name,\n",
    "    Description='PySpark job to extract Yelp review sentiment analysis',\n",
    "    Role=role, # you can pass your existing AWS Glue role here if you have used Glue before\n",
    "    ExecutionProperty={\n",
    "        'MaxConcurrentRuns': 1\n",
    "    },\n",
    "    Command={\n",
    "        'Name': 'glueetl',\n",
    "        'ScriptLocation': script_location\n",
    "    },\n",
    "    DefaultArguments={\n",
    "        '--job-language': 'python',\n",
    "    },\n",
    "    AllocatedCapacity=5,\n",
    "    Timeout=60,\n",
    ")\n",
    "glue_job_name = response['Name']\n",
    "print(glue_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aforementioned job will be executed now by calling `start_job_run` API. This API creates an immutable run/execution corresponding to the job definition created above. We will require the `job_run_id` for the particular job execution to check for status. We'll pass the data and model locations as part of the job execution parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_run_id = glue.start_job_run(JobName=job_name,\n",
    "                                       Arguments = {\n",
    "                                        '--S3_OUTPUT_BUCKET': s3_output_bucket,\n",
    "                                        '--S3_OUTPUT_KEY_PREFIX': s3_output_key_prefix,\n",
    "                                        '--DATABASE_NAME': database_name,\n",
    "                                        '--TABLE_NAME': table_name,\n",
    "                                        '--REGION': region\n",
    "                                       })['JobRunId']\n",
    "print(job_run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Glue Job status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will check for the job status to see if it has `succeeded`, `failed` or `stopped`. Once the job is succeeded, we have the transformed data into S3 in Parquet format which we will use to query with Athena and visualize with QuickSight. If the job fails, you can go to AWS Glue console, click on **Jobs** tab on the left, and from the page, click on this particular job and you will be able to find the CloudWatch logs (the link under **Logs**) link for these jobs which can help you to see what exactly went wrong in the job execution.\n",
    "\n",
    "[glue.get_job_run](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/glue.html#Glue.Client.get_job_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_run_status = glue.get_job_run(JobName=job_name,RunId=job_run_id)['JobRun']['JobRunState']\n",
    "while job_run_status not in ('FAILED', 'SUCCEEDED', 'STOPPED'):\n",
    "    job_run_status = glue.get_job_run(JobName=job_name,RunId=job_run_id)['JobRun']['JobRunState']\n",
    "    print (job_run_status)\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a [Glue Crawler](https://docs.aws.amazon.com/glue/latest/dg/add-crawler.html) to Discover the transformed data\n",
    "\n",
    "You can use a crawler to populate the AWS Glue Data Catalog with tables. This is the primary method used by most AWS Glue users. You add a crawler within your Data Catalog to traverse your data stores. The output of the crawler consists of one or more metadata tables that are defined in your Data Catalog. Extract, transform, and load (ETL) jobs that you define in AWS Glue use these metadata tables as sources and targets.\n",
    "\n",
    "[glue.create_crawler](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/glue.html#Glue.Client.create_crawler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parq_crawler_name = 'YelpCuratedCRawler'\n",
    "parq_crawler_path = 's3://'+bucket+'/yelp/parquet/'\n",
    "\n",
    "response = glue.create_crawler(\n",
    "    Name=parq_crawler_name,\n",
    "    Role=role,\n",
    "    DatabaseName=database_name,\n",
    "    Description='Crawler for the Parquet Yelp Reviews with Sentiment',\n",
    "    Targets={\n",
    "        'S3Targets': [\n",
    "            {\n",
    "                'Path': parq_crawler_path\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    SchemaChangePolicy={\n",
    "        'UpdateBehavior': 'UPDATE_IN_DATABASE',\n",
    "        'DeleteBehavior': 'DEPRECATE_IN_DATABASE'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the Glue Crawler\n",
    "\n",
    "Execute the cell below and browse to the Glue Crawler we created above. From there you will click the `Run Crawler` button to start the crawl of the raw Github activity data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Parquet Crawler: https://{0}.console.aws.amazon.com/glue/home?region={0}#crawler:name={1}\".format(region, parq_crawler_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Glue crawler status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler_status = glue.get_crawler(Name=parq_crawler_name)['Crawler']['State']\n",
    "while crawler_status not in ('READY'):\n",
    "    crawler_status = glue.get_crawler(Name=parq_crawler_name)['Crawler']['State']\n",
    "    print(crawler_status)\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View parquet results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('select * from yelp.parquet limit 100')\n",
    "\n",
    "df = as_pandas(cursor)\n",
    "df.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh\n",
    "import bokeh.io\n",
    "bokeh.io.output_notebook()\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import HoverTool\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "cursor.execute('select count(id) as cnt, sentiment from yelp.parquet group by sentiment')\n",
    "\n",
    "df = as_pandas(cursor)\n",
    "df.head(10)\n",
    "\n",
    "#p = figure(plot_width=900, plot_height=400, x_axis_type='datetime',x_axis_label='count', y_axis_label='sentiment')\n",
    "#p.circle(source=df, x='cnt', y='sentiment', color='black')\n",
    "\n",
    "#show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = glue.delete_crawler(Name=parq_crawler_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = glue.delete_job(JobName=glue_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = glue.delete_database(\n",
    "    CatalogId = account_id,\n",
    "    Name = database_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
