{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serverless Data Lake Ingestion with Kinesis\n",
    "\n",
    "In this notebook we will walk through the steps required to use the Kinesis suite of tools as a swiss army knife to land data into your Data Lake. We will simulate Apache logs from a web server that could be sent from the [Kinesis Logs Agent](https://github.com/awslabs/amazon-kinesis-agent) to an Kinesis Data Stream. Once the logs have been sent to Kinesis we can convert, transform, and persist the processed and raw logs in your Data Lake. Finally, we will also show the real-time aspect of Kinesis by using an enhanced fan-out consumer with Lambda to send 500 errors found in the stream to Slack. The diagram below depicts the solutions we will be creating below.\n",
    "\n",
    "![Kinesis Ingestion](../../docs/assets/images/kinesis-swiss-army.png)\n",
    "\n",
    "You will need a Slack account and web hook to complete this workshop. Create a Slack account [here](https://slack.com/get-started). Once you have the acocunt created you will need to create a [Slack Channel](https://get.slack.help/hc/en-us/articles/201402297-Create-a-channel) and add a [WebHook](https://get.slack.help/hc/en-us/articles/115005265063-Incoming-WebHooks-for-Slack) to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import json\n",
    "import time\n",
    "import project_path\n",
    "import getpass\n",
    "\n",
    "from lib import workshop\n",
    "\n",
    "cfn = boto3.client('cloudformation')\n",
    "logs = boto3.client('logs')\n",
    "firehose = boto3.client('firehose')\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# General variables for the region and account id for the location of the resources being created\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "delivery_stream_name = 'dc-demo-firehose'\n",
    "kdg_stack = 'kinesis-data-generator-cognito'\n",
    "kdg_username = 'admin'\n",
    "\n",
    "slack_web_hook = '{{SlackURL}}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Create S3 Bucket](https://docs.aws.amazon.com/AmazonS3/latest/gsg/CreatingABucket.html)\n",
    "\n",
    "We will create an S3 bucket that will be used throughout the workshop for storing our data.\n",
    "\n",
    "[s3.create_bucket](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.create_bucket) boto3 documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = workshop.create_bucket(region, session, 'demo-')\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter password for the Kinesis Data Generator (KDG)\n",
    "\n",
    "**Must contain only alphanumeric characters with at least one capital letter and one number.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "password = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Launch Kinesis Data Generator](https://awslabs.github.io/amazon-kinesis-data-generator/web/help.html)\n",
    "\n",
    "*** This is optional if you haven't already launched the KDG in your account ***\n",
    "\n",
    "The Amazon Kinesis Data Generator (KDG) makes it easy to send data to Kinesis Streams or Kinesis Firehose. Learn how to use the tool and create templates for your records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfn_kdg_template = 'https://s3-{0}.amazonaws.com/kinesis-helpers/cognito-setup.json'.format(region)\n",
    "print(cfn_kdg_template)\n",
    "\n",
    "response = cfn.create_stack(\n",
    "    StackName=kdg_stack,\n",
    "    TemplateURL=cfn_kdg_template,\n",
    "    Capabilities = [\"CAPABILITY_NAMED_IAM\"],\n",
    "    Parameters=[\n",
    "        {\n",
    "            'ParameterKey': 'Password',\n",
    "            'ParameterValue': password\n",
    "        },\n",
    "        {\n",
    "            'ParameterKey': 'Username',\n",
    "            'ParameterValue': kdg_username\n",
    "        }\n",
    "    ]    \n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = cfn.describe_stacks(\n",
    "    StackName=kdg_stack\n",
    ")\n",
    "\n",
    "while response['Stacks'][0]['StackStatus'] != 'CREATE_COMPLETE':\n",
    "    print('Not yet complete.')\n",
    "    time.sleep(30)\n",
    "    response = cfn.describe_stacks(\n",
    "        StackName=kdg_stack\n",
    "    )\n",
    "    \n",
    "for output in response['Stacks'][0]['Outputs']:\n",
    "    if (output['OutputKey'] == 'KinesisDataGeneratorUrl'):\n",
    "        print(output['OutputValue'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload [CloudFormation](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/GettingStarted.html) template\n",
    "\n",
    "In the interest of time we will leverage CloudFormation to launch many of the supporting resources needed for utilizing Kinesis Data Firehose to store the raw data, pre-process the incoming data, and store the curated data in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_file = 'cfn/kinesis-swiss-army.yaml'\n",
    "session.resource('s3').Bucket(bucket).Object(demo_file).upload_file(demo_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfn_template = 'https://s3-{0}.amazonaws.com/{1}/{2}'.format(region, bucket, demo_file)\n",
    "print(cfn_template)\n",
    "\n",
    "stack_name = 'kinesis-swiss-army'\n",
    "response = cfn.create_stack(\n",
    "    StackName=stack_name,\n",
    "    TemplateURL=cfn_template,\n",
    "    Capabilities = [\"CAPABILITY_NAMED_IAM\"],\n",
    "    Parameters=[\n",
    "        {\n",
    "            'ParameterKey': 'SlackWebHookUrl',\n",
    "            'ParameterValue': slack_web_hook\n",
    "        }\n",
    "    ]    \n",
    "    \n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = cfn.describe_stacks(\n",
    "    StackName=stack_name\n",
    ")\n",
    "\n",
    "while response['Stacks'][0]['StackStatus'] != 'CREATE_COMPLETE':\n",
    "    print('Not yet complete.')\n",
    "    time.sleep(30)\n",
    "    response = cfn.describe_stacks(\n",
    "        StackName=stack_name\n",
    "    )\n",
    "\n",
    "for output in response['Stacks'][0]['Outputs']:\n",
    "    if (output['OutputKey'] == 'FirehoseExecutionRole'):\n",
    "        firehose_arn = output['OutputValue']\n",
    "        print('Firehose Role Arn: {0}'.format(firehose_arn))\n",
    "    if (output['OutputKey'] == 'LambdaPreProcessArn'):\n",
    "        pre_processing_arn = output['OutputValue']\n",
    "        print('Lambda Pre Process Arn: {0}'.format(pre_processing_arn))\n",
    "    if (output['OutputKey'] == 'GlueDatabase'):\n",
    "        database = output['OutputValue']\n",
    "        print('Glue Database: {0}'.format(database))\n",
    "    if (output['OutputKey'] == 'RawTable'):\n",
    "        raw_table = output['OutputValue']\n",
    "        print('Glue Raw Table: {0}'.format(raw_table))\n",
    "    if (output['OutputKey'] == 'CuratedTable'):\n",
    "        curated_table = output['OutputValue']\n",
    "        print('Glue Curated Table: {0}'.format(curated_table))\n",
    "    if (output['OutputKey'] == 'WeblogsBucket'):\n",
    "        event_bucket = output['OutputValue']\n",
    "        print('S3 Weblogs Bucket: {0}'.format(event_bucket))\n",
    "    if (output['OutputKey'] == 'FirehoseLogGroup'):\n",
    "        cloudwatch_logs_group_name = output['OutputValue']\n",
    "        print('CloudWatch Logs Group: {0}'.format(cloudwatch_logs_group_name))\n",
    "    if (output['OutputKey'] == 'KinesisEventStream'):\n",
    "        event_stream_arn = output['OutputValue']\n",
    "        print('Kinesis Event Stream: {0}'.format(event_stream_arn))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Create the Kinesis Firehose we will use to send Apache Logs to our Data Lake](https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html)\n",
    "\n",
    "Amazon Kinesis Data Firehose is a fully managed service for delivering real-time streaming data to destinations such as Amazon Simple Storage Service (Amazon S3), Amazon Redshift, Amazon Elasticsearch Service (Amazon ES), and Splunk. Kinesis Data Firehose is part of the Kinesis streaming data platform, along with Kinesis Data Streams, Kinesis Video Streams, and Amazon Kinesis Data Analytics. With Kinesis Data Firehose, you don't need to write applications or manage resources. You configure your data producers to send data to Kinesis Data Firehose, and it automatically delivers the data to the destination that you specified. You can also configure Kinesis Data Firehose to transform your data before delivering it.\n",
    "\n",
    "In this example, we will create custom S3 prefixes for when the data lands in S3. This will allow us to precreate the partitions that will be cataloged in the Glue Data Catalog. To find more information follow this [link](https://docs.aws.amazon.com/firehose/latest/dev/s3-prefixes.html)\n",
    "\n",
    "[firehose.create_delivery_stream](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/firehose.html#Firehose.Client.create_delivery_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = firehose.create_delivery_stream(\n",
    "    DeliveryStreamName=delivery_stream_name,\n",
    "    DeliveryStreamType='KinesisStreamAsSource',\n",
    "    KinesisStreamSourceConfiguration={\n",
    "        'KinesisStreamARN': event_stream_arn,\n",
    "        'RoleARN': firehose_arn\n",
    "    },\n",
    "    ExtendedS3DestinationConfiguration={\n",
    "        'RoleARN': firehose_arn,\n",
    "        'BucketARN': 'arn:aws:s3:::' + event_bucket,\n",
    "        'Prefix': 'weblogs/processed/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/hour=!{timestamp:HH}/',\n",
    "        'ErrorOutputPrefix': 'weblogs/failed/!{firehose:error-output-type}/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/hour=!{timestamp:HH}/',\n",
    "        'BufferingHints': {\n",
    "            'SizeInMBs': 128,\n",
    "            'IntervalInSeconds': 60\n",
    "        },\n",
    "        'CompressionFormat': 'UNCOMPRESSED',\n",
    "        'EncryptionConfiguration': {\n",
    "            'NoEncryptionConfig': 'NoEncryption'\n",
    "        },\n",
    "        'CloudWatchLoggingOptions': {\n",
    "            'Enabled': True,\n",
    "            'LogGroupName': cloudwatch_logs_group_name,\n",
    "            'LogStreamName': 'ingestion_stream'\n",
    "        },\n",
    "        'ProcessingConfiguration': {\n",
    "            'Enabled': True,\n",
    "            'Processors': [\n",
    "                {\n",
    "                    'Type': 'Lambda',\n",
    "                    'Parameters': [\n",
    "                        {\n",
    "                            'ParameterName': 'LambdaArn',\n",
    "                            'ParameterValue': '{0}:$LATEST'.format(pre_processing_arn)\n",
    "                        },\n",
    "                        {\n",
    "                            'ParameterName': 'NumberOfRetries',\n",
    "                            'ParameterValue': '1'\n",
    "                        },\n",
    "                        {\n",
    "                            'ParameterName': 'RoleArn',\n",
    "                            'ParameterValue': firehose_arn\n",
    "                        },\n",
    "                        {\n",
    "                            'ParameterName': 'BufferSizeInMBs',\n",
    "                            'ParameterValue': '3'\n",
    "                        },\n",
    "                        {\n",
    "                            'ParameterName': 'BufferIntervalInSeconds',\n",
    "                            'ParameterValue': '60'\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        'S3BackupMode': 'Enabled',\n",
    "        'S3BackupConfiguration': {\n",
    "            'RoleARN': firehose_arn,\n",
    "            'BucketARN': 'arn:aws:s3:::' + event_bucket,\n",
    "            'Prefix': 'weblogs/raw/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/hour=!{timestamp:HH}/',\n",
    "            'ErrorOutputPrefix': 'weblogs/failed/!{firehose:error-output-type}/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/hour=!{timestamp:HH}/',\n",
    "            'BufferingHints': {\n",
    "                'SizeInMBs': 128,\n",
    "                'IntervalInSeconds': 60\n",
    "            },\n",
    "            'CompressionFormat': 'UNCOMPRESSED',\n",
    "            'EncryptionConfiguration': {\n",
    "                'NoEncryptionConfig': 'NoEncryption'\n",
    "            },\n",
    "            'CloudWatchLoggingOptions': {\n",
    "                'Enabled': True,\n",
    "                'LogGroupName': cloudwatch_logs_group_name,\n",
    "                'LogStreamName': 'raw_stream'\n",
    "            }\n",
    "        },\n",
    "        'DataFormatConversionConfiguration': {\n",
    "            'SchemaConfiguration': {\n",
    "                'RoleARN': firehose_arn,\n",
    "                'DatabaseName': database,\n",
    "                'TableName': curated_table,\n",
    "                'Region': region,\n",
    "                'VersionId': 'LATEST'\n",
    "            },\n",
    "            'InputFormatConfiguration': {\n",
    "                'Deserializer': {\n",
    "                    'OpenXJsonSerDe': {}\n",
    "                }\n",
    "            },\n",
    "            'OutputFormatConfiguration': {\n",
    "                'Serializer': {\n",
    "                    'ParquetSerDe': {}\n",
    "                }\n",
    "            },\n",
    "            'Enabled': True\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for the Kinesis Firehose to become 'Active'\n",
    "The Kinesis Firehose Delivery Stream is in the process of being created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = firehose.describe_delivery_stream(\n",
    "    DeliveryStreamName=delivery_stream_name\n",
    ")\n",
    "\n",
    "status = response['DeliveryStreamDescription']['DeliveryStreamStatus']\n",
    "print(status)\n",
    "\n",
    "while status == 'CREATING':\n",
    "    time.sleep(30)\n",
    "    response = firehose.describe_delivery_stream(\n",
    "        DeliveryStreamName=delivery_stream_name\n",
    "    )\n",
    "    status = response['DeliveryStreamDescription']['DeliveryStreamStatus']\n",
    "    print(status)\n",
    "\n",
    "print('Kinesis Firehose created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = cfn.describe_stacks(\n",
    "    StackName=kdg_stack\n",
    ")\n",
    "\n",
    "for output in response['Stacks'][0]['Outputs']:\n",
    "    if (output['OutputKey'] == 'KinesisDataGeneratorUrl'):\n",
    "        print(output['OutputValue'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be sending simulated apache logs to the Kinesis Data Stream and land the data in the raw and processed prefixes in the data lake.\n",
    "\n",
    "``` json\n",
    "{{internet.ip}} - - [{{date.now(\"DD/MMM/YYYY:HH:mm:ss ZZ\")}}] \"{{random.weightedArrayElement({\"weights\":[0.6,0.1,0.1,0.2],\"data\":[\"GET\",\"POST\",\"DELETE\",\"PUT\"]})}} {{random.arrayElement([\"/list\",\"/wp-content\",\"/wp-admin\",\"/explore\",\"/search/tag/list\",\"/app/main/posts\",\"/posts/posts/explore\"])}} HTTP/1.1\" {{random.weightedArrayElement({\"weights\": [0.9,0.04,0.02,0.04], \"data\":[\"200\",\"404\",\"500\",\"301\"]})}} {{random.number(10000)}} \"-\" \"{{internet.userAgent}}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 rb s3://$event_bucket --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = cfn.delete_stack(StackName=stack_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waiter = cfn.get_waiter('stack_delete_complete')\n",
    "waiter.wait(\n",
    "    StackName=stack_name\n",
    ")\n",
    "\n",
    "print('The wait is over for {0}'.format(stack_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = firehose.delete_delivery_stream(\n",
    "    DeliveryStreamName=delivery_stream_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 rb s3://$bucket --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
